{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud from Enron Email #\n",
    "## Data Anylyst Nano Degree -- Project 5 ##\n",
    "### Richard Lorenzo ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview ##\n",
    "This project studies financial data and email data from the public Enron Dataset.  Using Machine Learning analysis, I will predict \"Persons of Interest\" (POI) based on this data.\n",
    "\n",
    "1. I began with Exploratory Data Analysis (EDA) to validate the data, look for general trends and identify outliers.\n",
    "\n",
    "2. After cleaning the dataset, I will summarize the data and provide overall metrics.\n",
    "\n",
    "3. I will descibe three machine learning analyses, the methods, and the results.\n",
    "\n",
    "4. An appendix is included with EDA plots used in step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: 'poi_id.py' ##\n",
    "The following Python program is the basis of the analysis. It is show below with the output.  However, I will review and restate each result in this report.\n",
    "\n",
    "The 'poi_id.py' proram uses the following helper code and data were provided by Udacity for this project:\n",
    "- 'feature_format.py'\n",
    "- 'tester.py'\n",
    "- 'final_project_dataset.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rl1891\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\rl1891\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Outliers for 'TOTAL' and 'THE TRAVEL AGENCY IN THE PARK'\n",
      "\n",
      "Total number of data points (observations) : 144\n",
      "Numer of POI observations : 18\n",
      "Number of non-POI obseervations : 126\n",
      "\n",
      "Total features available :  21\n",
      "\n",
      "Available Feature sorted by NaNs\n",
      "                               0\n",
      "loan_advances                141\n",
      "director_fees                128\n",
      "restricted_stock_deferred    127\n",
      "deferral_payments            106\n",
      "deferred_income               96\n",
      "long_term_incentive           79\n",
      "bonus                         63\n",
      "to_messages                   58\n",
      "from_this_person_to_poi       58\n",
      "from_messages                 58\n",
      "shared_receipt_with_poi       58\n",
      "from_poi_to_this_person       58\n",
      "from_this_person_to_poi_pct   58\n",
      "from_poi_to_this_person_pct   58\n",
      "other                         53\n",
      "expenses                      50\n",
      "salary                        50\n",
      "exercised_stock_options       43\n",
      "restricted_stock              35\n",
      "total_payments                21\n",
      "total_stock_value             19\n",
      "email_address                  0\n",
      "poi                            0\n",
      "\n",
      "******************\n",
      " Select K Best + Gaussian NB Pipeline\n",
      "\n",
      "Processing time: 3.312 s\n",
      "                       Feature   p value\n",
      "0                        bonus  0.000011\n",
      "1      exercised_stock_options  0.000002\n",
      "2                       salary  0.000035\n",
      "3            total_stock_value  0.000002\n",
      "4  from_this_person_to_poi_pct  0.000084\n",
      "\n",
      "Pipeline(steps=[('SKB', SelectKBest(k=5, score_func=<function f_classif at 0x000000000877FEB8>)), ('classifier', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.84893\tPrecision: 0.41910\tRecall: 0.34450\tF1: 0.37816\tF2: 0.35722\n",
      "\tTotal predictions: 15000\tTrue positives:  689\tFalse positives:  955\tFalse negatives: 1311\tTrue negatives: 12045\n",
      "\n",
      "\n",
      "******************\n",
      " Select K Best + DecisionTree Pipeline\n",
      "\n",
      "Processing time: 272.378 s\n",
      "                       Feature  Importance   p value\n",
      "0                        bonus    0.138586  0.000011\n",
      "1              deferred_income    0.000000  0.000922\n",
      "2      exercised_stock_options    0.124407  0.000002\n",
      "3          long_term_incentive    0.000000  0.001994\n",
      "4                       salary    0.068504  0.000035\n",
      "5            total_stock_value    0.262293  0.000002\n",
      "6  from_this_person_to_poi_pct    0.406210  0.000084\n",
      "\n",
      "Pipeline(steps=[('SKB', SelectKBest(k=7, score_func=<function f_classif at 0x000000000877FEB8>)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=6,\n",
      "            min_samples_split=8, min_weight_fraction_leaf=0, presort=False,\n",
      "            random_state=42, splitter='best'))])\n",
      "\tAccuracy: 0.84867\tPrecision: 0.37546\tRecall: 0.20350\tF1: 0.26394\tF2: 0.22402\n",
      "\tTotal predictions: 15000\tTrue positives:  407\tFalse positives:  677\tFalse negatives: 1593\tTrue negatives: 12323\n",
      "\n",
      "\n",
      "******************\n",
      " Gaussian NB w/ manual Features\n",
      "\n",
      "Manually Selected Features :  ['from_poi_to_this_person_pct', 'salary', 'deferred_income', 'exercised_stock_options', 'expenses', 'total_stock_value']\n",
      "Processing time: 0.001 s\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.87273\tPrecision: 0.53031\tRecall: 0.39800\tF1: 0.45473\tF2: 0.41890\n",
      "\tTotal predictions: 15000\tTrue positives:  796\tFalse positives:  705\tFalse negatives: 1204\tTrue negatives: 12295\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rl1891\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [9] are constant.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, load_classifier_and_data, test_classifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from itertools import compress\n",
    "from pprint import pprint\n",
    "from IPython.display import display\n",
    "from time import time\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary'] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Convert to Pandas\n",
    "df = pd.DataFrame.from_records(list(data_dict.values()))\n",
    "employees = pd.Series(list(data_dict.keys()))\n",
    "# set the index of df to be the employees series:\n",
    "df.set_index(employees, inplace=True)\n",
    "# Convert Numeric Column types\n",
    "# Convert Financial Columns \n",
    "df[['bonus','deferral_payments',\n",
    "    'deferred_income', 'director_fees','exercised_stock_options','expenses',\n",
    "    'loan_advances', 'long_term_incentive', 'loan_advances','other', \n",
    "    'restricted_stock', 'restricted_stock_deferred','salary','total_payments', \n",
    "    'total_stock_value']] = df[['bonus','deferral_payments',\n",
    "    'deferred_income', 'director_fees','exercised_stock_options','expenses',\n",
    "    'loan_advances', 'long_term_incentive', 'loan_advances','other', \n",
    "    'restricted_stock', 'restricted_stock_deferred','salary','total_payments', \n",
    "    'total_stock_value']].apply(pd.to_numeric,errors='coerce')\n",
    "# Convert email columns\n",
    "df[['from_messages', 'from_poi_to_this_person','from_this_person_to_poi',\n",
    "    'shared_receipt_with_poi','to_messages']] = df[['from_messages', \n",
    "    'from_poi_to_this_person','from_this_person_to_poi',\n",
    "    'shared_receipt_with_poi',\n",
    "    'to_messages']].apply(pd.to_numeric,errors='coerce')\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "df.drop('TOTAL', inplace = True)    # Remove Outliers\n",
    "df.drop('THE TRAVEL AGENCY IN THE PARK', inplace = True)    # Remove Outliers\n",
    "print \"Removed Outliers for 'TOTAL' and 'THE TRAVEL AGENCY IN THE PARK'\\n\"\n",
    "\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "df['from_poi_to_this_person_pct'] = \\\n",
    "    df['from_poi_to_this_person'] / df['to_messages']\n",
    "df['from_this_person_to_poi_pct'] = \\\n",
    "    df['from_this_person_to_poi'] / df['from_messages']\n",
    "\n",
    "nan_observations = {}\n",
    "for column in df:\n",
    "    nan_observations[column] = df[column].isnull().sum()\n",
    "\n",
    "df.fillna(value=0,inplace = True)\n",
    "\n",
    "### Data Exploration\n",
    "print \"Total number of data points (observations) :\", len(df.index)\n",
    "print \"Numer of POI observations :\", len(df['poi'][df['poi']])\n",
    "print \"Number of non-POI obseervations :\", len(df['poi'][df['poi'] == False])\n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "df.to_csv('enron_for_eda.txt')\n",
    "df_dict = df.to_dict('index')\n",
    "my_dataset = df_dict\n",
    "\n",
    "features_list = ['poi',\n",
    " 'bonus',\n",
    " 'deferral_payments',\n",
    " 'deferred_income',\n",
    " 'director_fees',\n",
    " 'exercised_stock_options',\n",
    " 'expenses',\n",
    " 'from_messages',\n",
    " 'from_poi_to_this_person',\n",
    " 'from_this_person_to_poi',\n",
    " 'loan_advances',\n",
    " 'long_term_incentive',\n",
    " 'other',\n",
    " 'restricted_stock',\n",
    " 'restricted_stock_deferred',\n",
    " 'salary',\n",
    " 'shared_receipt_with_poi',\n",
    " 'to_messages',\n",
    " 'total_payments',\n",
    " 'total_stock_value',\n",
    " 'from_poi_to_this_person_pct',\n",
    " 'from_this_person_to_poi_pct'\n",
    "]\n",
    "\n",
    "# create a list of features without 'poi' which is a label\n",
    "features_no_poi = list(features_list) # copy the feature list\n",
    "features_no_poi.pop(0)  # remove 'poi' from the list\n",
    "print \"\\nTotal features available : \", len(features_no_poi)\n",
    "print\n",
    "print \"Available Feature sorted by NaNs\"\n",
    "df_nans = pd.DataFrame.from_dict(nan_observations, orient = 'index')\n",
    "pprint(df_nans.sort_values(by = 0, ascending=False))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# Implement Modeling Pipeline with \"Select K Best\" and \"Naive Bayes\"\n",
    "####################################################################\n",
    "print \"\\n******************\\n Select K Best + Gaussian NB Pipeline\\n\"\n",
    "t0 = time()\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "k_range = range(2,10)\n",
    "params = {'SKB__k' : k_range }\n",
    "pipeline = Pipeline([('SKB', SelectKBest()), ('classifier', GaussianNB())])\n",
    "cv = StratifiedShuffleSplit(labels, 100, test_size=0.2, random_state=60)\n",
    "gs = GridSearchCV(pipeline, params, cv=cv, scoring=\"f1_weighted\")\n",
    "gs.fit(features, labels)\n",
    "clf = gs.best_estimator_\n",
    "\n",
    "# Print the selected features and pvalues\n",
    "print \"Processing time:\", round(time()-t0, 3), \"s\"\n",
    "k_best_support = clf.named_steps['SKB'].get_support(False).tolist()\n",
    "df_selected_features1 = pd.DataFrame(\n",
    "    {'Feature': list(compress(features_no_poi, k_best_support)),\n",
    "     'p value': list(compress(clf.named_steps['SKB'].pvalues_,k_best_support))\n",
    "    })\n",
    "pprint(df_selected_features1)\n",
    "print\n",
    "\n",
    "# Test the results\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "clf, dataset, feature_list = load_classifier_and_data()\n",
    "test_classifier(clf, dataset, feature_list)\n",
    "\n",
    "#####################################################################\n",
    "# Implement Modeling Pipeline with \"Select K Best\" and \"DecisionTree\"\n",
    "####################################################################\n",
    "print \"\\n******************\\n Select K Best + DecisionTree Pipeline\\n\"\n",
    "t0 = time()\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "k_range = range(2,8)\n",
    "params = {'SKB__k' : k_range,\n",
    "          \"dt__min_samples_leaf\": [2, 4, 6],\n",
    "          \"dt__min_samples_split\": [8, 10, 12],\n",
    "          \"dt__min_weight_fraction_leaf\": [0, 0.1],\n",
    "          \"dt__criterion\": [\"gini\", \"entropy\"],\n",
    "          \"dt__random_state\": [42, 46]}\n",
    "          \n",
    "pipeline = Pipeline([('SKB', SelectKBest()),('dt', DecisionTreeClassifier())])\n",
    "cv = StratifiedShuffleSplit(labels, 100, test_size=0.2, random_state=60)\n",
    "gs = GridSearchCV(pipeline, params, cv=cv, scoring=\"f1_weighted\")\n",
    "gs.fit(features, labels)\n",
    "clf = gs.best_estimator_\n",
    "\n",
    "# Print the selected features, pvalues, and DT Importances\n",
    "print \"Processing time:\", round(time()-t0, 3), \"s\"\n",
    "k_best_support = clf.named_steps['SKB'].get_support(False).tolist()\n",
    "df_selected_features2 = pd.DataFrame(\n",
    "    {'Feature': list(compress(features_no_poi, k_best_support)),\n",
    "    'p value': list(compress(clf.named_steps['SKB'].pvalues_,k_best_support)),\n",
    "    'Importance' : clf.named_steps['dt'].feature_importances_.tolist()\n",
    "    })\n",
    "pprint(df_selected_features2)\n",
    "print\n",
    "\n",
    "# Test the results\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "clf, dataset, feature_list = load_classifier_and_data()\n",
    "test_classifier(clf, dataset, feature_list)\n",
    "\n",
    "#####################################################################\n",
    "# Implement \"Naive Bayes\" with Manually Selected Features\n",
    "#####################################################################\n",
    "print \"\\n******************\\n Gaussian NB w/ manual Features\\n\"\n",
    "features_list_manual = ['poi',\n",
    " 'from_poi_to_this_person_pct',\n",
    " 'salary',  'deferred_income', \n",
    " 'exercised_stock_options',  'expenses', \n",
    " 'total_stock_value']\n",
    "\n",
    "print \"Manually Selected Features : \", features_list_manual[1:]\n",
    "\n",
    "t0 = time()\n",
    "data = featureFormat(my_dataset, features_list_manual, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "features_train, features_test, labels_train, labels_test\\\n",
    "    = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "clf = GaussianNB()\n",
    "print \"Processing time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "# Test the results\n",
    "dump_classifier_and_data(clf, my_dataset, features_list_manual)\n",
    "clf, dataset, features_list_manual = load_classifier_and_data()\n",
    "test_classifier(clf, dataset, features_list_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis##\n",
    "\n",
    "I immediately converted the dataset to a Pandas DataFrame to simplify analysis and manipulation.  I exported it as a '.csv' file to import it to RStudio for EDA.  The plots are included in the appendix.  I prepared several 'ggpairs' plots to look for trends, correlations, and get an overall feel for the dataset.  Next, I plotted several promising features against each other to look for outliers.\n",
    "\n",
    "The 'TOTAL' obersvation was obvious in these plots.  I removed 'TOTAL' because it is not a valid observation.  Also, the 'THE TRAVEL AGENCY IN THE PARK' is also invalid since it cannot be considered a POI.  There were several other extreme values, but since they appear to valid amounts, I did not remove them.\n",
    "\n",
    "### Data Charactistics: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points (observations) : 144\n",
      "Numer of POI observations : 18\n",
      "Number of non-POI obseervations : 126\n"
     ]
    }
   ],
   "source": [
    "print \"Total number of data points (observations) :\", len(df.index)\n",
    "print \"Numer of POI observations :\", len(df['poi'][df['poi']])\n",
    "print \"Number of non-POI obseervations :\", len(df['poi'][df['poi'] == False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many features contained NaN data and had to be filled in with zeros.  The following chart lists the features sorted with the ones with the most NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loan_advances</th>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director_fees</th>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferral_payments</th>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferred_income</th>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_term_incentive</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonus</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to_messages</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_messages</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_this_person_to_poi_pct</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_poi_to_this_person_pct</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expenses</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restricted_stock</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_payments</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_stock_value</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>email_address</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poi</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0\n",
       "loan_advances                141\n",
       "director_fees                128\n",
       "restricted_stock_deferred    127\n",
       "deferral_payments            106\n",
       "deferred_income               96\n",
       "long_term_incentive           79\n",
       "bonus                         63\n",
       "to_messages                   58\n",
       "from_this_person_to_poi       58\n",
       "from_messages                 58\n",
       "shared_receipt_with_poi       58\n",
       "from_poi_to_this_person       58\n",
       "from_this_person_to_poi_pct   58\n",
       "from_poi_to_this_person_pct   58\n",
       "other                         53\n",
       "expenses                      50\n",
       "salary                        50\n",
       "exercised_stock_options       43\n",
       "restricted_stock              35\n",
       "total_payments                21\n",
       "total_stock_value             19\n",
       "email_address                  0\n",
       "poi                            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_nans.sort_values(by = 0, ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding New Features ##\n",
    "\n",
    "I added the following two new features:\n",
    "\n",
    "- from_poi_to_this_person_pct\n",
    "- from_this_person_to_poi_pct\n",
    "\n",
    "The original email counts to/from POIs can be improved by looking at a ratio of their overall emails.  For example, a person who sends many emails including a few to POIs is less interesting than a person who sends fewer emails, but to the same number of POIs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the Classifiers##\n",
    "\n",
    "### Evaluation Metrics ###\n",
    "\n",
    "Because only (14) of the (144) observations are POIs, percision and recall are the most appropriate metrics.  Accuaracy is less important because an alogorithm that always guesses 'not a POI' will have a 87.5% accuracy.  \n",
    "\n",
    "Percision is a measure of result relevancy.  Recall is a measure of how many truly relevant results are returned. High precision relates to a low false positive rate, and high recall relates to a low false negative rate.\n",
    "\n",
    "For example, a precision of 0.22604 and a recall of 0.39500 means the classifier has too many false positives, but it has an acceptably low number of false negatives.\n",
    "\n",
    "The final analysis has a precision of 0.53031 and a recall: 0.39800 which means both a low false positive rate and a low false negtive negative rate.\n",
    "\n",
    "\n",
    "### Validation ###\n",
    "\n",
    "Validation is important to ensure the classier is both responsive and does not over-fit the testing data set. Classifiers that are not learning from the training data will not adapt to the test data.  Classifiers that over-fit the trainnig data will not perform well with different test data.\n",
    "\n",
    "With only (144) observations the dataset is small, and splitting the already small dataset in testing and training sets is problematic and causes large variations depending on the testing vs. training data.  I used the StratifiedShuffleSplit method in SKLearn's Cross Validation package to multiply overlapping testing/training splits and to average the results.\n",
    "\n",
    "### Steps taken for coding the classifiers ###\n",
    "The first Machines Learning classifier I used is present last because it was the most accurate. \n",
    "\n",
    "I started by simply coding several classifiers with default tunning and guessed at several features.  I tried GaussianNB, DecisionTree, K Nearest Neighbors, and Support Vector Machines.  GausssianNB clearly produced the best results.  (The results from K Nearest Neighbors and Support Vector Machines and not included in poi_id.py or discussed further because their results were poor.)  I then manually removed features until the metrics dropped and added new features in.  I only kept them in if they improved the metrices.  The results of this manual GaussianNB alogrithm are present later in the report.\n",
    "\n",
    "### Scaling ###\n",
    "Since I only used GaussianNB and Decision Tree Algorithms, I did not need to scale my features.  However, scaling could be easily added to the pipelines presented if someone want to try a different alogrithm.\n",
    "\n",
    "### Performance ###\n",
    "The processing time to run the Select K-Best and GaussianNB classifier was 3.454 seconds.  This was slightly slower performance than the final analisys.\n",
    "\n",
    "## Select K-Best and GaussianNB##\n",
    "Hoping to improve on my manual feature selection, and keeping the promising GaussianNB classifier, I coded a pipeline usinf Select K Best and GridSearchCV using the GaussianNB classifier.  The GridSearchCV also implemented the StratifiedShuffleSplit cross validation.  GaussianNB does not have any tunable parameters, but I hoped Sleect K Best could identify beter features.\n",
    "\n",
    "The pipeline identified the following features with their associated p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>p value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bonus</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exercised_stock_options</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salary</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>total_stock_value</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from_this_person_to_poi_pct</td>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Feature   p value\n",
       "0                        bonus  0.000011\n",
       "1      exercised_stock_options  0.000002\n",
       "2                       salary  0.000035\n",
       "3            total_stock_value  0.000002\n",
       "4  from_this_person_to_poi_pct  0.000084"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_selected_features1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results / metrics are:\n",
    "```\n",
    "Pipeline(steps=[('SKB', SelectKBest(k=5, score_func=<function f_classif at 0x0000000008AD8EB8>)), \n",
    "            ('classifier', GaussianNB(priors=None))])\n",
    "\n",
    "    Accuracy: 0.84893   Precision: 0.41910\t  Recall: 0.34450\tF1: 0.37816\t  F2: 0.35722\n",
    "\n",
    "    Total predictions: 15000\t\n",
    "    True positives:  689\tFalse positives:  955\tFalse negatives: 1311\tTrue negatives: 12045\n",
    "```\n",
    "\n",
    "These results still meet the specified 0.3 for percison and recall, but they are less than the manual analysis to be presented later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select K-Best and the Decision Tree Classifier ##\n",
    "\n",
    "Eventhough the untuned Decision Tree classifier performed worse than NB initially, I coded a pipeline for Select K-Best and Decision Tree and improve the result through tuning.  The GridSearchCV also implemented the StratifiedShuffleSplit cross validation.\n",
    "\n",
    "The DT tunable paramters and the values tried are:\n",
    "```\n",
    "    min_samples_leaf         : 2, 4, 6\n",
    "    min_samples_split        : 8, 10, 12\n",
    "    min_weight_fraction_leaf : 0, 0.1\n",
    "    criterion                : gini, entropy\n",
    "    random_state             : 42, 46\n",
    "```\n",
    "\n",
    "Parameter tuning with StratifiedShuffleSplit cross validation is important because it ensures optimum performance with checking that the classifier is not over-fit.  GridSearchCV for parameter tuning, executes the DT classifier for each permutation of the above parameters.  The executed classifier with the best metric and the best combination of tunable parmeters is assign to the \"clf\" object, and is the result of the GridSearchCV.  The best parameters are:\n",
    "\n",
    "- min_samples_leaf=6\n",
    "- min_samples_split=8\n",
    "- min_weight_fraction_leaf=0\n",
    "- criterion='entropy'\n",
    "- random_state=42\n",
    "\n",
    "The processing time for the Select-K-Best and DecisionTree Classifier with paramet tuning and StratifiedShuffleSplit cross validation was 274.249 s.  By far, this method had the worst CPU performance.\n",
    "\n",
    "This time, Select K-Best pick the following features.  Also, since DecisionTree also outputs feature importance values, these are presented here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "      <th>p value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bonus</td>\n",
       "      <td>0.138586</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deferred_income</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exercised_stock_options</td>\n",
       "      <td>0.124407</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>long_term_incentive</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>salary</td>\n",
       "      <td>0.068504</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>total_stock_value</td>\n",
       "      <td>0.262293</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>from_this_person_to_poi_pct</td>\n",
       "      <td>0.406210</td>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Feature  Importance   p value\n",
       "0                        bonus    0.138586  0.000011\n",
       "1              deferred_income    0.000000  0.000922\n",
       "2      exercised_stock_options    0.124407  0.000002\n",
       "3          long_term_incentive    0.000000  0.001994\n",
       "4                       salary    0.068504  0.000035\n",
       "5            total_stock_value    0.262293  0.000002\n",
       "6  from_this_person_to_poi_pct    0.406210  0.000084"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_selected_features2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results / metrics are:\n",
    "```\n",
    "Pipeline(steps=[('SKB', SelectKBest(k=7, score_func=<function f_classif at 0x0000000008AD8EB8>)), \n",
    "            ('dt', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=6,\n",
    "            min_samples_split=8, min_weight_fraction_leaf=0, presort=False,\n",
    "            random_state=42, splitter='best'))])\n",
    "            \n",
    "\tAccuracy: 0.84867\tPrecision: 0.37546\t Recall: 0.20350\tF1: 0.26394\t  F2: 0.22402\n",
    "\tTotal predictions: 15000\t\n",
    "    True positives:  407\tFalse positives:  677\tFalse negatives: 1593\tTrue negatives: 12323\n",
    "```\n",
    "\n",
    "The recall metric does not meet the required 0.3, and the metrics are less than the GaussianNB algoirthm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis ##\n",
    "### GaussianNB with Manual Feature Selection ###\n",
    "\n",
    "I manually coded a GaussianNB classifier and guessed at likely features from my EDA plots.  The, I removed features until the metrics dropped and added new features in. I only kept them in if they improved the metrices. \n",
    "\n",
    "Since I determined the features using trial and error, I can only speculate why these features produced the best results:\n",
    "\n",
    "- \"from_poi_to_this_person_pct\" is likely a strong feature because, by definition, it is linked to known POIs. It likely allows the classifiers to correctly include POIs for individuals with a higher percentage of their emails are with POIs.\n",
    "\n",
    "- \"salary\" is likely a strong feature because many POIs have high salaries amd most low-salary employees are not POIs.\n",
    "\n",
    "- \"deferred_income\" and \"expenses\" are likely strong features because employees who did not have access to the \"perks\" were likely not tied to the fraud and this assist the classier.\n",
    "\n",
    "- \"total_stock_value\" is likely a strong feature because employees with large stock amounts are incented to \"bend\" the rules, and coversely, employees without large stock holding are not.\n",
    "\n",
    "- \"exercised_stock_options\" is a likely strong feature because these people may have known about the fraud, and were cashing out their options.  Employees without knowledge of the fraud would be more likely to have kept their options. \n",
    "\n",
    "The manually selected features witn GaussianNB had the best CPU performance time of 0.001 seconds.\n",
    "\n",
    "My final features were:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>from_poi_to_this_person_pct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>salary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deferred_income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exercised_stock_options</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>expenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>total_stock_value</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0\n",
       "0  from_poi_to_this_person_pct\n",
       "1                       salary\n",
       "2              deferred_income\n",
       "3      exercised_stock_options\n",
       "4                     expenses\n",
       "5            total_stock_value"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(features_list_manual[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results / metrics are:\n",
    "```\n",
    "GaussianNB(priors=None)\n",
    "\n",
    "\tAccuracy: 0.87273\tPrecision: 0.53031\tRecall: 0.39800\tF1: 0.45473\tF2: 0.41890\n",
    "\tTotal predictions: 15000\t\n",
    "    True positives:  796\tFalse positives:  705\tFalse negatives: 1204\tTrue negatives: 12295\n",
    "```\n",
    "\n",
    "These are the best results achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPENDIX ##\n",
    "The EDA Plots are include in the file: enron_eda.pdf and shown below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=./enron_eda.pdf width=950 height=1500></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src=./enron_eda.pdf width=950 height=1500></iframe>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
